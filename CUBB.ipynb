{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CUBB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saivarshittha/fine-grained-image-classification/blob/main/CUBB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ufNAdPZj4_1I",
        "outputId": "49d4851a-1277-4100-bdae-c42e22940f86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')\n",
        "%cd /gdrive\n",
        "!ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "/gdrive\n",
            "'My Drive'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yuG5jeWl5N1J",
        "outputId": "6f2879fb-89f1-43ca-8983-93cde6723590",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /gdrive/My\\ Drive\n",
        "# !ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLU_lyCt5x2T"
      },
      "source": [
        "#!tar -zxvf CUB_200_2011.tgz"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpqBKDxSClLG"
      },
      "source": [
        "batch_size = 28"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujnrBy98bk2h"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from collections import OrderedDict"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjZ6DcuY5B8J"
      },
      "source": [
        "### CPU vs GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pffk_t1O5EV5"
      },
      "source": [
        "# Find the device available to use using torch library\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Move model to the device specified above\n",
        "# model.to(device)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRXIz9mO5T1S"
      },
      "source": [
        "# Dataset Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybRjH9ZXyehJ"
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from torchvision.datasets.folder import default_loader\n",
        "from torchvision.datasets.utils import download_url\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "\n",
        "class Cub2011(Dataset):\n",
        "    base_folder = 'CUB_200_2011/images'\n",
        "    url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
        "    filename = 'CUB_200_2011.tgz'\n",
        "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
        "\n",
        "    def __init__(self, root, train=True, loader=default_loader, download=False):\n",
        "        self.root = os.path.expanduser(root)\n",
        "        self.transform = transforms.Compose([transforms.Resize(255), \n",
        "                                       transforms.CenterCrop(224),  \n",
        "                                       transforms.RandomHorizontalFlip(),\n",
        "                                       transforms.ToTensor(), \n",
        "                                       transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])]) \n",
        "        self.loader = default_loader\n",
        "        self.train = train\n",
        "        self._load_metadata()\n",
        "\n",
        "    def _load_metadata(self):\n",
        "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
        "                             names=['img_id', 'filepath'])\n",
        "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
        "                                         sep=' ', names=['img_id', 'target'])\n",
        "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
        "                                       sep=' ', names=['img_id', 'is_training_img']) \n",
        "\n",
        "        data = images.merge(image_class_labels, on='img_id')\n",
        "\n",
        "        self.data = data.merge(train_test_split, on='img_id')\n",
        "\n",
        "        if self.train:\n",
        "            self.data = self.data[self.data.is_training_img == 1]\n",
        "        else:\n",
        "            self.data = self.data[self.data.is_training_img == 0]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data.iloc[idx]\n",
        "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
        "        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n",
        "        img = self.loader(path)\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb-4bc0xcZSd"
      },
      "source": [
        "train_dataset = Cub2011('/gdrive/My Drive')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vtlcydpqm9mP"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJNjVcB-nR3e",
        "outputId": "49e660fb-78d9-4918-a6ce-04ef139f9d6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(train_dataset)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5994"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LDoo3C3dbMj"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=28)\n",
        "batch = next(iter(train_loader))"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEO0UhHugOMd",
        "outputId": "bd514dd6-d0d2-411b-d96f-8b195c7a8953",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "batch[0].size()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([28, 3, 224, 224])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_Zty2QmQsHJ"
      },
      "source": [
        "## Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYtP4UxWoe58"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        resnet = torchvision.models.resnet101(pretrained=True)  # pretrained ImageNet ResNet-101\n",
        "        print(resnet)\n",
        "        # Remove linear and pool layers (since we're not doing classification)\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        modules1 = list(resnet.children())[:-4]\n",
        "        self.resnet1 = nn.Sequential(*modules1)\n",
        "\n",
        "        #self.drop1 = nn.Dropout2d(p=0.5)\n",
        "        #self.conv1d = nn.Conv2d(2048,1024, (1,1), stride=1)\n",
        "        self.conv1d = nn.Conv2d(2048,256, 1, stride=1,dilation = 1,padding =0)\n",
        "        self.conv1d_1 = nn.Conv2d(2048,256, 1, stride=1,dilation = 1,padding = 0)\n",
        "        self.conv1d_2 = nn.Conv2d(2048,256, 1, stride=1,dilation = 1,padding = 0)  # all above 3 are same?\n",
        "\n",
        "        self.conv2d_1 = nn.Conv2d(256,256, 3, stride=1,dilation = 1,padding = 1)\n",
        "        self.conv2d_2 = nn.Conv2d(256,256, 5, stride=1,dilation = 1,padding = 2)\n",
        "        self.conv2d_3 = nn.Conv2d(256,256, 1, stride=1,dilation = 1,padding = 0)\n",
        "\n",
        "        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((7,7)),nn.Conv2d(2048, 256,1,stride=1,dilation = 1,padding =0),nn.BatchNorm2d(256),nn.ReLU())\n",
        "        ## converted (8,8)->(7,7)\n",
        "\n",
        "        self.drop2 = nn.Dropout2d(p=0.3)\n",
        "\n",
        "        self.conv1_2d = nn.Conv2d(1024,512,1 , stride=1,dilation = 1,padding =0)\n",
        "        self.bn1 = nn.BatchNorm2d(1024)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.drop3 = nn.Dropout2d(p=0.5)\n",
        "\n",
        "        self.conv2_1d = nn.Conv2d(512,512, 1, stride=1,dilation = 1,padding =0)\n",
        "        self.conv2_2d = nn.Conv2d(1024,512, 3, stride=1,dilation = 1,padding =1)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(401408, 200)\n",
        "        )\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        #self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "        self.fine_tune()\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out1 = self.resnet1(images)\n",
        "        # print(\"out size = \",out.shape)  # (batch_size, 2048, image_size/32, image_size/32)  \n",
        "        # print(out1.shape)\n",
        "        #out = F.interpolate(out, size=32, mode='bilinear', align_corners=True)\n",
        "        #x = torch.cat((out, out1), dim=-3)\n",
        "\n",
        "        #out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        x_1 = self.conv1d(out)\n",
        "        x_2 = self.conv1d_1(out)\n",
        "        x_3 = self.conv1d_2(out)\n",
        "\n",
        "        x_2_1 = self.conv2d_1(x_2)\n",
        "        x_3_1 = self.conv2d_2(x_3)\n",
        "        # print(x_1.shape, x_2.shape, x_3.shape)\n",
        "        #x_4 = self.conv1d_3(out)\n",
        "\n",
        "        x_5 = self.global_avg_pool(out)\n",
        "        # print(\"shape of x_5 = \",x_5.shape)\n",
        "        x_5_1 = self.conv2d_3(x_5)\n",
        "        #x_6 = F.interpolate(x_5, size=8, mode='bilinear', align_corners=True)\n",
        "\n",
        "        # print(x_1.shape,x_2_1.shape,x_3_1.shape,x_5_1.shape)\n",
        "        x_6 = torch.cat((x_1, x_2_1, x_3_1, x_5_1), dim=-3)\n",
        "        # print(\"shape of x_6 = \",x_6.shape)\n",
        "        x_7 = self.conv1_2d(x_6)\n",
        "        # print(\"shape of x_7 = \",x_7.shape)\n",
        "        x_8 = F.interpolate(x_7, size=28, mode='bilinear', align_corners=True) ## changed from 32 to 28\n",
        "        #print(x_7.shape)   \n",
        "        # print(\"shape of x_8 = \",x_8.shape)\n",
        "        x_9 = self.conv2_1d(out1)\n",
        "        # print(\"shape of x_9 = \",x_9.shape)\n",
        "        y = torch.cat((x_8, x_9), dim=-3)   \n",
        "        #print(out.shape, x_1.shape, x_2.shape, x_3.shape,x_5.shape, x_5_1.shape, x_6.shape,x_8.shape, x_9.shape,y.shape)  \n",
        "        #y = self.drop2(z)\n",
        "        #y = self.conv1_2d(y)\n",
        "        y = self.bn1(y)\n",
        "        y = self.relu(y)\n",
        "        y = self.drop3(y)\n",
        "        y = self.conv2_2d(y)\n",
        "        #print(out.shape, out1.shape, x_6.shape,x_8.shape, x_9.shape,y.shape)\n",
        "        # print(\"y shape before = \",y.shape)\n",
        "        y = y.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        # print(\"shape of y after =\",y.shape)\n",
        "        y = y.contiguous()\n",
        "        y = y.view(y.size(0), -1)\n",
        "        y = self.classifier(y)\n",
        "        # n_inputs = self.fc.in_features\n",
        "        #print (y.shape)\n",
        "        return y\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdBhejKOa0d8"
      },
      "source": [
        "net = Encoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZwuqi3GQzHp"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPJZvSbNsHK_"
      },
      "source": [
        "@torch.no_grad()\n",
        "def get_all_preds(model, loader):\n",
        "    all_preds = torch.tensor([])\n",
        "    for batch in loader:\n",
        "        images, labels = batch\n",
        "\n",
        "        preds = model(images)\n",
        "        all_preds = torch.cat(\n",
        "            (all_preds, preds)\n",
        "            ,dim=0\n",
        "        )\n",
        "    return all_preds"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izrCH7D0rsHo"
      },
      "source": [
        "# with torch.no_grad():\n",
        "#   train_preds = get_all_preds(net,train_loader)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "558Fq6mR98Y2"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac1NK6aQ-YfA",
        "outputId": "f0af4c30-95b4-4edb-a301-a8d14861ced5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "for epoch in range(2):\n",
        "  running_loss = 0.0\n",
        "  j=0\n",
        "  for i,data in enumerate(train_loader,0):\n",
        "    images,labels = data\n",
        "    print(j)\n",
        "    j = j+1\n",
        "    preds = net(images)\n",
        "    # labels=labels.unsqueeze(dim=0)\n",
        "    optimizer.zero_grad()\n",
        "    # print(\"shape of preds\",preds.size())\n",
        "    # print(\"labels size\",labels.size())\n",
        "    # print(labels)\n",
        "    loss = criterion(preds,labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    running_loss += loss.item()\n",
        "    if(i%2000 ==1999):\n",
        "      print(\"xyz\")\n",
        "print(\"done\")\n",
        "\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-26591d16e03d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# labels=labels.unsqueeze(dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-1d784e771117>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (batch_size, 2048, image_size/32, image_size/32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;31m# print(\"out size = \",out.shape)  # (batch_size, 2048, image_size/32, image_size/32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownsample\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             self.weight, self.bias, bn_training, exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2014\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   2015\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2016\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2017\u001b[0m     )\n\u001b[1;32m   2018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFjgX_ORxGCZ"
      },
      "source": [
        "# for epoch in range(2):\n",
        "#   running_loss = 0.0\n",
        "#   for i,data in enumerate(train_loader,0):\n",
        "#     images,labels = data\n",
        "#     optimizer.zero_grad()\n",
        "#     preds = net(images)\n",
        "#     print(\"shape of preds\",preds.size())\n",
        "#     print(labels.size())\n",
        "#     # print(labels)\n",
        "#     loss = criterion(preds,labels.view(-1))\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "\n",
        "#     running_loss += loss.item()\n",
        "#     if(i%2000 ==1999):\n",
        "#       print(\"xyz\")\n",
        "# print(\"done\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KTIbf-eq4NX"
      },
      "source": [
        "preds"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcVBlpdrbTP_"
      },
      "source": [
        "# for epoch in range(2):\n",
        "#   for batch in train_loader:\n",
        "#     images,labels = batch\n",
        "#     preds = net(images)\n",
        "#     # loss = F.cross_entropy(preds,labels)\n",
        "#     optimiser.zero_grad()\n",
        "#     loss.backward()\n",
        "#     optimizer.step()\n",
        "#     all_preds = torch.cat(all_preds,preds,dim = 0)\n",
        "#   print(\"epoch\")\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvxsCF-q_6DS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}